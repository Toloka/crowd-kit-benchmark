{"results": {"label_aggregation_benchmark.Relevance2.time_dawid_skene": {"result": [5.124003944499691], "stats": [{"ci_99": [0.16759306947506047, 10.080414819524318], "q_25": 5.074439835749445, "q_75": 5.173568053249937, "min": 5.024875726999198, "max": 5.223132162000184, "mean": 5.124003944499691, "std": 0.09912821750049261, "repeat": 2, "number": 1}]}, "label_aggregation_benchmark.Relevance2.time_gold_majority_vote": {"result": [0.8495929345003788], "stats": [{"ci_99": [0.7922935730002791, 0.9125747070002035], "q_25": 0.8334322525006428, "q_75": 0.8770287687500513, "min": 0.7922935730002791, "max": 0.9125747070002035, "mean": 0.8502283541001816, "std": 0.035522014304117146, "repeat": 10, "number": 1}]}, "label_aggregation_benchmark.Relevance2.time_majority_vote": {"result": [0.6161698600003547], "stats": [{"ci_99": [0.5838920180003697, 0.6859348119996866], "q_25": 0.5925451250004699, "q_75": 0.6491964130002543, "min": 0.5838920180003697, "max": 0.6859348119996866, "mean": 0.6231025577002584, "std": 0.0326771196410729, "repeat": 10, "number": 1}]}, "label_aggregation_benchmark.Relevance2.time_mmsr": null, "label_aggregation_benchmark.Relevance2.time_wawa": {"result": [1.6712365319995115], "stats": [{"ci_99": [1.5494714460000978, 1.7149879210001018], "q_25": 1.6395314112498909, "q_75": 1.6945389014999819, "min": 1.5494714460000978, "max": 1.7149879210001018, "mean": 1.6558281387999159, "std": 0.05109794550703217, "repeat": 10, "number": 1}]}, "label_aggregation_benchmark.Relevance2.time_zbs": {"result": [10.219761091500004], "stats": [{"ci_99": [3.2822644165180463, 17.157257766481955], "q_25": 10.150386124750185, "q_75": 10.289136058249824, "min": 10.081011158000365, "max": 10.358511024999643, "mean": 10.219761091500004, "std": 0.13874993349963916, "repeat": 2, "number": 1}]}, "label_aggregation_benchmark.Relevance2.peakmem_dawid_skene": 219701248, "label_aggregation_benchmark.Relevance2.peakmem_gold_majority_vote": 197160960, "label_aggregation_benchmark.Relevance2.peakmem_majority_vote": 205819904, "label_aggregation_benchmark.Relevance2.peakmem_mmsr": null, "label_aggregation_benchmark.Relevance2.peakmem_wawa": 217239552, "label_aggregation_benchmark.Relevance2.peakmem_zbs": 226422784, "label_aggregation_benchmark.Relevance2.track_accuracy_dawid_skene": 0.8106955055064987, "label_aggregation_benchmark.Relevance2.track_accuracy_gold_majority_vote": 0.8255779343188808, "label_aggregation_benchmark.Relevance2.track_accuracy_majority_vote": 0.7763667030459371, "label_aggregation_benchmark.Relevance2.track_accuracy_mmsr": null, "label_aggregation_benchmark.Relevance2.track_accuracy_wawa": null, "label_aggregation_benchmark.Relevance2.track_accuracy_zbs": 0.7733902172834607}, "params": {"arch": "x86_64", "cpu": "Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz", "machine": "sinosov-vm.sas.yp-c.yandex.net", "num_cpu": "16", "os": "Linux 4.19.100-23", "ram": "16GB", "python": "3.8"}, "requirements": {}, "commit_hash": "12a86c26e1c51d5549440f40628147b38f9e9fca", "date": 1626624486000, "env_name": "virtualenv-py3.8", "python": "3.8", "profiles": {}, "started_at": {"label_aggregation_benchmark.Relevance2.time_dawid_skene": 1632149166647, "label_aggregation_benchmark.Relevance2.time_gold_majority_vote": 1632149177131, "label_aggregation_benchmark.Relevance2.time_majority_vote": 1632149183604, "label_aggregation_benchmark.Relevance2.time_mmsr": 1632148994300, "label_aggregation_benchmark.Relevance2.time_wawa": 1632149188673, "label_aggregation_benchmark.Relevance2.time_zbs": 1632149200061, "label_aggregation_benchmark.Relevance2.peakmem_dawid_skene": 1632149086746, "label_aggregation_benchmark.Relevance2.peakmem_gold_majority_vote": 1632149092015, "label_aggregation_benchmark.Relevance2.peakmem_majority_vote": 1632149093173, "label_aggregation_benchmark.Relevance2.peakmem_mmsr": 1632149094131, "label_aggregation_benchmark.Relevance2.peakmem_wawa": 1632149154254, "label_aggregation_benchmark.Relevance2.peakmem_zbs": 1632149156114, "label_aggregation_benchmark.Relevance2.track_accuracy_dawid_skene": 1632149221828, "label_aggregation_benchmark.Relevance2.track_accuracy_gold_majority_vote": 1632149227197, "label_aggregation_benchmark.Relevance2.track_accuracy_majority_vote": 1632149228455, "label_aggregation_benchmark.Relevance2.track_accuracy_mmsr": 1632149229412, "label_aggregation_benchmark.Relevance2.track_accuracy_wawa": 1632149289577, "label_aggregation_benchmark.Relevance2.track_accuracy_zbs": 1632149289832}, "ended_at": {"label_aggregation_benchmark.Relevance2.time_dawid_skene": 1632149177131, "label_aggregation_benchmark.Relevance2.time_gold_majority_vote": 1632149183604, "label_aggregation_benchmark.Relevance2.time_majority_vote": 1632149188672, "label_aggregation_benchmark.Relevance2.time_mmsr": 1632149054404, "label_aggregation_benchmark.Relevance2.time_wawa": 1632149200060, "label_aggregation_benchmark.Relevance2.time_zbs": 1632149221827, "label_aggregation_benchmark.Relevance2.peakmem_dawid_skene": 1632149092014, "label_aggregation_benchmark.Relevance2.peakmem_gold_majority_vote": 1632149093173, "label_aggregation_benchmark.Relevance2.peakmem_majority_vote": 1632149094131, "label_aggregation_benchmark.Relevance2.peakmem_mmsr": 1632149154253, "label_aggregation_benchmark.Relevance2.peakmem_wawa": 1632149156114, "label_aggregation_benchmark.Relevance2.peakmem_zbs": 1632149166647, "label_aggregation_benchmark.Relevance2.track_accuracy_dawid_skene": 1632149227197, "label_aggregation_benchmark.Relevance2.track_accuracy_gold_majority_vote": 1632149228455, "label_aggregation_benchmark.Relevance2.track_accuracy_majority_vote": 1632149229412, "label_aggregation_benchmark.Relevance2.track_accuracy_mmsr": 1632149289577, "label_aggregation_benchmark.Relevance2.track_accuracy_wawa": 1632149289832, "label_aggregation_benchmark.Relevance2.track_accuracy_zbs": 1632149300567}, "benchmark_version": {"label_aggregation_benchmark.Relevance2.time_dawid_skene": "88f5ffd9d32ebcca57bcf651a025ad2a0b691ed8678ddfdc0b7b3c3706ae2841", "label_aggregation_benchmark.Relevance2.time_gold_majority_vote": "a941271c216476d18a69aa535cadb07952260a39843740fe0e24b39da38f2b87", "label_aggregation_benchmark.Relevance2.time_majority_vote": "bf7b878a140af9efeece34d841b277f355e06580d861c68bbc83c5778d378874", "label_aggregation_benchmark.Relevance2.time_mmsr": "5571c05342e9008d9a622d1fd39aab8313ac30d57a0bbc14f8a75fdecf325ec5", "label_aggregation_benchmark.Relevance2.time_wawa": "67cb33c8ffccda0337fde361b5ada33d055cbe734f2d5e18bb3b34175e5dae45", "label_aggregation_benchmark.Relevance2.time_zbs": "439d37a43e482da76ad622b5a5a193a8f7db820344721d397e88445def4fb83e", "label_aggregation_benchmark.Relevance2.peakmem_dawid_skene": "c4518a41f907d2fdca9ec6e25a60a9b7fd53da0b1bd0118501903650bb5f1638", "label_aggregation_benchmark.Relevance2.peakmem_gold_majority_vote": "5f5916fabc3b14c3bdabc854557c7ebafca1ee9217cff4cddabc8dd5df1da65e", "label_aggregation_benchmark.Relevance2.peakmem_majority_vote": "5f7d206e231aae89f9cd72f2db890099a89e82eef9e2820a0c0d8f4e3ddb5efe", "label_aggregation_benchmark.Relevance2.peakmem_mmsr": "586fc489203a8698ebf76b1fdd33e672de89daed052f26ebc0ee51f3d2b5925c", "label_aggregation_benchmark.Relevance2.peakmem_wawa": "74851db6b519ddfce3cc70216af3da6a65142006c943caeca214b211ac785d41", "label_aggregation_benchmark.Relevance2.peakmem_zbs": "0b38906e5d5119c3c36a58dc75c41c24b4e4b0ecc614743f1618fa43fd33f108", "label_aggregation_benchmark.Relevance2.track_accuracy_dawid_skene": "7e0a657652a7e72d8cb6aef9507ae81cad50d30d418a12ad20e99c3d3c50ef1a", "label_aggregation_benchmark.Relevance2.track_accuracy_gold_majority_vote": "69bd13cf276103040ca16849ee8bdec6c75438df3e592dd47e03e7d129c76a5d", "label_aggregation_benchmark.Relevance2.track_accuracy_majority_vote": "7916e4d5988a8dcb77986cebe3cfbced48c6f347e996ee2048a11a02eac51ada", "label_aggregation_benchmark.Relevance2.track_accuracy_mmsr": "860fd1d24b3cc508a9f9d2c2e0a54b6545bd630b580f2595b093bb9b15592b36", "label_aggregation_benchmark.Relevance2.track_accuracy_wawa": "285b344d14541c0120f5aaa5f6085e6f0ec03e37d865a01c5dc15204452ae1b1", "label_aggregation_benchmark.Relevance2.track_accuracy_zbs": "4efd0601ce4690ee1622934fd2590487d14f30ff25b99adc81121bf4f75afefd"}, "version": 1}